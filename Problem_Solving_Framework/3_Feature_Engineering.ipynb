{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of creating, transforming, or selecting features to improve the performance of machine learning models. It involves domain knowledge and creativity to derive meaningful features from raw data. Here are several techniques for feature engineering:\n",
    "\n",
    "### 1. **Mathematical Transformations**\n",
    "\n",
    "- **Log Transformation**: Apply a logarithmic function to features to reduce skewness and handle exponential growth.\n",
    "  ```python\n",
    "  df['log_feature'] = np.log1p(df['feature'])\n",
    "  ```\n",
    "\n",
    "- **Square Root Transformation**: Useful for reducing the impact of outliers and stabilizing variance.\n",
    "  ```python\n",
    "  df['sqrt_feature'] = np.sqrt(df['feature'])\n",
    "  ```\n",
    "\n",
    "- **Polynomial Features**: Create interaction terms and higher-order features by raising features to a power or multiplying them together.\n",
    "  ```python\n",
    "  from sklearn.preprocessing import PolynomialFeatures\n",
    "  poly = PolynomialFeatures(degree=2)\n",
    "  X_poly = poly.fit_transform(X)\n",
    "  ```\n",
    "\n",
    "### 2. **Binning and Discretization**\n",
    "\n",
    "- **Binning**: Convert continuous features into categorical bins. This can help with capturing non-linear relationships.\n",
    "  ```python\n",
    "  df['binned_feature'] = pd.cut(df['feature'], bins=5, labels=False)\n",
    "  ```\n",
    "\n",
    "- **Quantile Binning**: Divide features into quantiles, ensuring that each bin has approximately the same number of observations.\n",
    "  ```python\n",
    "  df['quantile_binned'] = pd.qcut(df['feature'], q=4, labels=False)\n",
    "  ```\n",
    "\n",
    "### 3. **Encoding Categorical Variables**\n",
    "\n",
    "- **One-Hot Encoding**: Convert categorical variables into binary columns for each category.\n",
    "  ```python\n",
    "  df_encoded = pd.get_dummies(df, columns=['categorical_feature'])\n",
    "  ```\n",
    "\n",
    "- **Label Encoding**: Convert categorical variables into integer labels.\n",
    "  ```python\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  le = LabelEncoder()\n",
    "  df['encoded_feature'] = le.fit_transform(df['categorical_feature'])\n",
    "  ```\n",
    "\n",
    "- **Frequency Encoding**: Encode categories based on the frequency of occurrence in the dataset.\n",
    "  ```python\n",
    "  freq_encoding = df['categorical_feature'].value_counts().to_dict()\n",
    "  df['freq_encoded'] = df['categorical_feature'].map(freq_encoding)\n",
    "  ```\n",
    "\n",
    "- **Target Encoding**: Encode categories based on the mean of the target variable for each category.\n",
    "  ```python\n",
    "  mean_target = df.groupby('categorical_feature')['target'].mean()\n",
    "  df['target_encoded'] = df['categorical_feature'].map(mean_target)\n",
    "  ```\n",
    "\n",
    "### 4. **Feature Extraction**\n",
    "\n",
    "- **Text Features**: Extract features from text data using methods like Bag of Words, TF-IDF, or word embeddings.\n",
    "  ```python\n",
    "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  X_text = vectorizer.fit_transform(df['text_column'])\n",
    "  ```\n",
    "\n",
    "- **Date-Time Features**: Extract useful features from date-time data, such as day of the week, month, or time of day.\n",
    "  ```python\n",
    "  df['day_of_week'] = df['date_column'].dt.dayofweek\n",
    "  df['month'] = df['date_column'].dt.month\n",
    "  df['hour'] = df['date_column'].dt.hour\n",
    "  ```\n",
    "\n",
    "### 5. **Aggregation and Grouping**\n",
    "\n",
    "- **Aggregate Features**: Create new features by aggregating data within groups, such as mean, sum, or count.\n",
    "  ```python\n",
    "  df_grouped = df.groupby('group_feature').agg({'numeric_feature': ['mean', 'sum', 'count']})\n",
    "  df_grouped.columns = ['mean_numeric', 'sum_numeric', 'count_numeric']\n",
    "  df = df.merge(df_grouped, on='group_feature', how='left')\n",
    "  ```\n",
    "\n",
    "- **Rolling Statistics**: Compute rolling statistics like moving average, sum, or standard deviation over a window of time or indices.\n",
    "  ```python\n",
    "  df['rolling_mean'] = df['numeric_feature'].rolling(window=3).mean()\n",
    "  ```\n",
    "\n",
    "### 6. **Feature Scaling and Normalization**\n",
    "\n",
    "- **Standardization**: Scale features to have zero mean and unit variance.\n",
    "  ```python\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  df_scaled = scaler.fit_transform(df[['feature1', 'feature2']])\n",
    "  ```\n",
    "\n",
    "- **Min-Max Normalization**: Scale features to a specified range, usually [0, 1].\n",
    "  ```python\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "  scaler = MinMaxScaler()\n",
    "  df_normalized = scaler.fit_transform(df[['feature']])\n",
    "  ```\n",
    "\n",
    "### 7. **Feature Selection**\n",
    "\n",
    "- **Feature Importance**: Use algorithms that provide feature importance scores (e.g., Random Forest, XGBoost) to select the most influential features.\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(X_train, y_train)\n",
    "  importances = model.feature_importances_\n",
    "  ```\n",
    "\n",
    "- **Recursive Feature Elimination (RFE)**: Recursively remove the least important features and refit the model.\n",
    "  ```python\n",
    "  from sklearn.feature_selection import RFE\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  model = LogisticRegression()\n",
    "  rfe = RFE(model, n_features_to_select=5)\n",
    "  X_rfe = rfe.fit_transform(X, y)\n",
    "  ```\n",
    "\n",
    "### 8. **Domain-Specific Features**\n",
    "\n",
    "- **Domain Knowledge**: Utilize specific knowledge about the domain to create features that capture important aspects of the data.\n",
    "  - For example, in finance, features like “debt-to-income ratio” or in health, “BMI” can be created from existing features.\n",
    "\n",
    "### 9. **Feature Engineering Automation**\n",
    "\n",
    "- **Featuretools**: An open-source library for automated feature engineering that can generate features using deep feature synthesis.\n",
    "  ```python\n",
    "  import featuretools as ft\n",
    "  es = ft.EntitySet(id='data')\n",
    "  es = es.add_dataframe(dataframe_name='df', dataframe=df, index='index')\n",
    "  feature_matrix, feature_defs = ft.dfs(entityset=es, target_dataframe_name='df')\n",
    "  ```\n",
    "\n",
    "### Practical Tips\n",
    "\n",
    "- **Iterate**: Feature engineering is an iterative process. Continuously refine and test features to improve model performance.\n",
    "- **Validate**: Always validate the impact of new features using cross-validation or hold-out validation sets to avoid overfitting.\n",
    "- **Visualize**: Use visualizations to understand the distribution and relationships of features, which can guide feature engineering.\n",
    "\n",
    "By applying these techniques thoughtfully, you can enhance the predictive power of your models and gain better insights from your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
